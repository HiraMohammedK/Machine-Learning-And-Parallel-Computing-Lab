# -*- coding: utf-8 -*-
"""ML_models_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JLanT0r96EVuhQ2jAkO6VMwUNlTwuJ8E
"""

import pandas as pd
from keras.datasets import mnist
import tensorflow
import sklearn.metrics as metrics
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,classification_report,ConfusionMatrixDisplay
from sklearn.metrics import precision_recall_curve, roc_curve
from sklearn.preprocessing import LabelEncoder
from imblearn.metrics import specificity_score
from imblearn.metrics import sensitivity_score
from sklearn.metrics import cohen_kappa_score
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics
from tensorflow import keras


(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()


X_train = X_train.reshape(-1, 784)
X_test = X_test.reshape(-1, 784)


X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0


model = LogisticRegression(max_iter=1000000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
kappa = cohen_kappa_score(y_test, y_pred)
specificity = specificity_score(y_test, y_pred, average='macro')
sensitive = sensitivity_score(y_test, y_pred, average='macro')
mae_lR = metrics.mean_absolute_error(y_test, y_pred)
mse_LR = metrics.mean_squared_error(y_test, y_pred)

print(f"Accuracy Logistic Regression: {accuracy:.2f}")
print(f"Precision Logistic Regression: {precision:.2f}")
print(f"Recall Logistic Regression: {recall:.2f}")
print(f"F1 score Logistic Regression: {f1:.2f}")
print('Cohens kappa: %f' % kappa)
print(f"Specificity Logistic Regression: {specificity:.2f}")
print(f"Sensitivity Logistic Regression: {sensitive:.2f}")
print(f"MAE Logistic Regression: {mae_lR:.2f}")
print(f"MSE Logistic Regression: {mse_LR:.2f}")

cm = confusion_matrix(y_test, y_pred)
cm_df = pd.DataFrame(cm, index=range(10), columns=range(10))

plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_recall_curve
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score
pred_prob = model.predict_proba(X_test)
y_pred_lr= model.predict_proba(X_test)
random_probs = [0 for i in range(len(y_test))]
fpr = {}
tpr = {}
thresh ={}
n_class = 10
for i in range(n_class):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_lr[:,i], pos_label=i)

colors = ['orange', 'green', 'blue', 'red', 'purple', 'brown','yellow','black','pink','grey']
for i in range(n_class):
  plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()
plt.savefig('Multiclass ROC',dpi=300);



n_classes = 10
average_auc = 0.0
y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
for i in range(n_classes):
    average_auc += roc_auc_score(y_test_binarized[:, i], pred_prob[:, i])

average_auc /= n_classes

print("Average AUC:", average_auc)



precision = dict()
recall = dict()

for i in range(n_class):
    precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], pred_prob[:, i])

def print_recalls_precision(recall, precision, title):
    plt.figure(figsize=(6, 6))
    for i in range(n_class):
        plt.plot(recall[i], precision[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
    plt.xlabel("Recall", fontsize=16)
    plt.ylabel("Precision", fontsize=16)
    plt.title("Precision vs Recall plot - {0}".format(title), fontsize=16)
    plt.axis([0, 1, 0, 1])
    plt.legend(loc="best")
    plt.show()

print_recalls_precision(recall, precision, "Logistic Regression")

# Train a DecisionTreeClassifier on the training data
model2 = DecisionTreeClassifier()
model2.fit(X_train, y_train)
# Make predictions on the test data
y_pred = model2.predict(X_test)
# Evaluate the performance of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
kappa = cohen_kappa_score(y_test, y_pred)
specificity = specificity_score(y_test, y_pred, average='macro')
sensitive = sensitivity_score(y_test, y_pred, average='macro')
mae_lR = metrics.mean_absolute_error(y_test, y_pred)
mse_LR = metrics.mean_squared_error(y_test, y_pred)

print(f"Accuracy Decision Tree: {accuracy:.2f}")
print(f"Precision Decision Tree: {precision:.2f}")
print(f"Recall Decision Tree: {recall:.2f}")
print(f"F1 score Decision Tree: {f1:.2f}")
print('Cohens kappa: %f' % kappa)
print(f"Specificity Decision Tree: {specificity:.2f}")
print(f"Sensitivity Decision Tree: {sensitive:.2f}")
print(f"MAE Decision Tree: {mae_lR:.2f}")
print(f"MSE Decision Tree: {mse_LR:.2f}")


# Plot Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
cm_df = pd.DataFrame(cm, index=range(10), columns=range(10))

plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_recall_curve
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score
pred_prob = model2.predict_proba(X_test)
y_pred_lr= model2.predict_proba(X_test)
random_probs = [0 for i in range(len(y_test))]
fpr = {}
tpr = {}
thresh ={}
n_class = 10
for i in range(n_class):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_lr[:,i], pos_label=i)

colors = ['orange', 'green', 'blue', 'red', 'purple', 'brown','yellow','black','pink','grey']
for i in range(n_class):
  plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()
plt.savefig('Multiclass ROC',dpi=300);



n_classes = 10
average_auc = 0.0
y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
for i in range(n_classes):
    average_auc += roc_auc_score(y_test_binarized[:, i], pred_prob[:, i])

average_auc /= n_classes

print("Average AUC:", average_auc)



precision = dict()
recall = dict()

for i in range(n_class):
    precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], pred_prob[:, i])

def print_recalls_precision(recall, precision, title):
    plt.figure(figsize=(6, 6))
    for i in range(n_class):
        plt.plot(recall[i], precision[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
    plt.xlabel("Recall", fontsize=16)
    plt.ylabel("Precision", fontsize=16)
    plt.title("Precision vs Recall plot - {0}".format(title), fontsize=16)
    plt.axis([0, 1, 0, 1])
    plt.legend(loc="best")
    plt.show()

print_recalls_precision(recall, precision, "Decision Tree")

from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score, auc
from sklearn.preprocessing import label_binarize

model3 = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)
model3.fit(X_train, y_train)
y_pred = model3.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
kappa = cohen_kappa_score(y_test, y_pred)
specificity = specificity_score(y_test, y_pred, average='macro')
sensitive = sensitivity_score(y_test, y_pred, average='macro')
mae_lR = metrics.mean_absolute_error(y_test, y_pred)
mse_LR = metrics.mean_squared_error(y_test, y_pred)

print(f"Accuracy : {accuracy:.2f}")
print(f"Precision : {precision:.2f}")
print(f"Recall : {recall:.2f}")
print(f"F1 score : {f1:.2f}")
print('Cohens kappa: %f' % kappa)
print(f"Specificity : {specificity:.2f}")
print(f"Sensitivity : {sensitive:.2f}")
print(f"MAE : {mae_lR:.2f}")
print(f"MSE : {mse_LR:.2f}")

cm = confusion_matrix(y_test, y_pred)
cm_df = pd.DataFrame(cm, index=range(10), columns=range(10))

plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()
report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")
y_score = model3.predict_proba(X_test)

report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_recall_curve
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score
pred_prob = model3.predict_proba(X_test)
y_pred_lr= model3.predict_proba(X_test)
random_probs = [0 for i in range(len(y_test))]
fpr = {}
tpr = {}
thresh ={}
n_class = 10
for i in range(n_class):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_lr[:,i], pos_label=i)

colors = ['orange', 'green', 'blue', 'red', 'purple', 'brown','yellow','black','pink','grey']
for i in range(n_class):
  plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()
plt.savefig('Multiclass ROC',dpi=300);



n_classes = 10
average_auc = 0.0

for i in range(n_classes):
    average_auc += roc_auc_score(y_test_binarized[:, i], pred_prob[:, i])

average_auc /= n_classes

print("Average AUC:", average_auc)

y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

precision = dict()
recall = dict()

for i in range(n_class):
    precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], pred_prob[:, i])

def print_recalls_precision(recall, precision, title):
    plt.figure(figsize=(6, 6))
    for i in range(n_class):
        plt.plot(recall[i], precision[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
    plt.xlabel("Recall", fontsize=16)
    plt.ylabel("Precision", fontsize=16)
    plt.title("Precision vs Recall plot - {0}".format(title), fontsize=16)
    plt.axis([0, 1, 0, 1])
    plt.legend(loc="best")
    plt.show()

print_recalls_precision(recall, precision, "Random Forest")

model5 = GaussianNB()
model5.fit(X_train, y_train)
y_pred = model5.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
kappa = cohen_kappa_score(y_test, y_pred)
specificity = specificity_score(y_test, y_pred, average='macro')
sensitive = sensitivity_score(y_test, y_pred, average='macro')
mae_lR = metrics.mean_absolute_error(y_test, y_pred)
mse_LR = metrics.mean_squared_error(y_test, y_pred)

print(f"Accuracy : {accuracy:.2f}")
print(f"Precision : {precision:.2f}")
print(f"Recall : {recall:.2f}")
print(f"F1 score : {f1:.2f}")
print('Cohens kappa: %f' % kappa)
print(f"Specificity : {specificity:.2f}")
print(f"Sensitivity : {sensitive:.2f}")
print(f"MAE : {mae_lR:.2f}")
print(f"MSE : {mse_LR:.2f}")

report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")

cm = confusion_matrix(y_test, y_pred)
cm_df = pd.DataFrame(cm, index=range(10), columns=range(10))

plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()
report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_recall_curve
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score
pred_prob = model5.predict_proba(X_test)
y_pred_lr= model5.predict_proba(X_test)
random_probs = [0 for i in range(len(y_test))]
fpr = {}
tpr = {}
thresh ={}
n_class = 10
for i in range(n_class):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_lr[:,i], pos_label=i)

colors = ['orange', 'green', 'blue', 'red', 'purple', 'brown','yellow','black','pink','grey']
for i in range(n_class):
  plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()
plt.savefig('Multiclass ROC',dpi=300);



n_classes = 10
average_auc = 0.0

for i in range(n_classes):
    average_auc += roc_auc_score(y_test_binarized[:, i], pred_prob[:, i])

average_auc /= n_classes

print("Average AUC:", average_auc)

y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

precision = dict()
recall = dict()

for i in range(n_class):
    precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], pred_prob[:, i])

def print_recalls_precision(recall, precision, title):
    plt.figure(figsize=(6, 6))
    for i in range(n_class):
        plt.plot(recall[i], precision[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
    plt.xlabel("Recall", fontsize=16)
    plt.ylabel("Precision", fontsize=16)
    plt.title("Precision vs Recall plot - {0}".format(title), fontsize=16)
    plt.axis([0, 1, 0, 1])
    plt.legend(loc="best")
    plt.show()

print_recalls_precision(recall, precision, "Naive Bayes")